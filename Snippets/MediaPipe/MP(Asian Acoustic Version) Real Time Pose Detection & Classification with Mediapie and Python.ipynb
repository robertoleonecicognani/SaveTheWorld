{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d9a10b",
   "metadata": {},
   "source": [
    "# **<center><font style=\"color:rgb(100,109,254)\">Real-Time 3D Pose Detection & Pose Classification with Mediapipe and Python</font> </center>**\n",
    "\n",
    "Pose Detection (also known as Pose Estimation) is a widely used computer vision task that enables you to predict humans poses in images or videos by localizing the key body joints (also reffered  as landmarks), these are elbows, shoulders, and knees, etc. \n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1hT-lhDvzft8vVQv6ObSok73h7A4l5CXf'>\n",
    "\n",
    "\n",
    "[MediaPipe](https://google.github.io/mediapipe/solutions/pose.html) provides a robust solution capable of predicting **thirty-three 3D landmarks** on a human body in real-time with high accuracy even on CPU. It utilizes a two-step machine learning pipeline, by using a detector it first localizes the person within the frame and then uses the pose landmarks detector to predict the  landmarks within the region of interest.\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1c1vFXlRbN9r4ONKIE3sEmaLsWEfm2vpF'>\n",
    "\n",
    "\n",
    "For the videos, the detector is used only for the very first frame and then the ROI is derived from the previous frameâ€™s pose landmarks using a tracking method. Also when the tracker loses track of the identify body pose presence in a frame, the detector is invoked again for the next frame which reduces the computation and latency. The image below shows the thirty-three pose landmarks along with their indexes.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=1CDO0KiXZEOuWc7xLEm7EFLLQf2hydCoI\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe46c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "from time import time\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e9b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Setting up the Pose function.\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.3, model_complexity=2)\n",
    "\n",
    "# Initializing mediapipe drawing class, useful for annotation.\n",
    "mp_drawing = mp.solutions.drawing_utils "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6325fad9",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Create a Pose Detection Function</font>**\n",
    "\n",
    "Now we will put all this together to create a function that will perform pose detection on an image and will visualize the results or return the results depending upon the passed arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c5628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectPose(image, pose, display=True):\n",
    "    '''\n",
    "    This function performs pose detection on an image.\n",
    "    Args:\n",
    "        image: The input image with a prominent person whose pose landmarks needs to be detected.\n",
    "        pose: The pose setup function required to perform the pose detection.\n",
    "        display: A boolean value that is if set to true the function displays the original input image, the resultant image, \n",
    "                 and the pose landmarks in 3D plot and returns nothing.\n",
    "    Returns:\n",
    "        output_image: The input image with the detected pose landmarks drawn.\n",
    "        landmarks: A list of detected landmarks converted into their original scale.\n",
    "    '''\n",
    "    \n",
    "    # Create a copy of the input image.\n",
    "    output_image = image.copy()\n",
    "    \n",
    "    # Convert the image from BGR into RGB format.\n",
    "    imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Perform the Pose Detection.\n",
    "    results = pose.process(imageRGB)\n",
    "    \n",
    "    # Retrieve the height and width of the input image.\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # Initialize a list to store the detected landmarks.\n",
    "    landmarks = []\n",
    "    \n",
    "    # Check if any landmarks are detected.\n",
    "    if results.pose_landmarks:\n",
    "    \n",
    "        # Draw Pose landmarks on the output image.\n",
    "        mp_drawing.draw_landmarks(image=output_image, landmark_list=results.pose_landmarks,\n",
    "                                  connections=mp_pose.POSE_CONNECTIONS)\n",
    "        \n",
    "        # Iterate over the detected landmarks.\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            \n",
    "            # Append the landmark into the list.\n",
    "            landmarks.append((int(landmark.x * width), int(landmark.y * height),\n",
    "                                  (landmark.z * width)))\n",
    "    \n",
    "    # Check if the original input image and the resultant image are specified to be displayed.\n",
    "    if display:\n",
    "    \n",
    "        # Display the original input image and the resultant image.\n",
    "        plt.figure(figsize=[22,22])\n",
    "        plt.subplot(121);plt.imshow(image[:,:,::-1]);plt.title(\"Original Image\");plt.axis('off');\n",
    "        plt.subplot(122);plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "        \n",
    "        # Also Plot the Pose landmarks in 3D.\n",
    "        mp_drawing.plot_landmarks(results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        \n",
    "    # Otherwise\n",
    "    else:\n",
    "        \n",
    "        # Return the output image and the found landmarks.\n",
    "        return output_image, landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0ca1e",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Pose Classification with Angle Heuristics</font>**\n",
    "\n",
    "We have learned to perform pose detection, now we will level up our game by also classifying different yoga poses using the calculated angles of various joints. We will first detect the pose landmarks and then use them to compute angles between joints and depending upon those angles we will recognize the yoga pose of the prominent person in an image.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1lhAcgq2jy5NavGQeYjTwwEAZIu6Aeypg' width=500>\n",
    "\n",
    "But this approach does have a drawback that limits its use to a controlled environment, the calculated angles vary with the angle between the person and the camera. So the person needs to be facing the camera straight to get the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a93a1",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Create a Function to Calculate Angle between Landmarks</font>**\n",
    "\n",
    "Now we will create a function that will be capable of calculating angles between three landmarks. The angle between landmarks? Do not get confused, as this is the same as calculating the angle between two lines. \n",
    "\n",
    "The first point (landmark) is considered as the starting point of the first line, the second point (landmark) is considered as the ending point of the first line and the starting point of the second line as well, and the third point (landmark) is considered as the ending point of the second line.\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1KSN2OnenNMZ7Jwai_E1jeWdP5Mzay3Ad' width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0c85cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAngle(landmark1, landmark2, landmark3):\n",
    "    '''\n",
    "    This function calculates angle between three different landmarks.\n",
    "    Args:\n",
    "        landmark1: The first landmark containing the x,y and z coordinates.\n",
    "        landmark2: The second landmark containing the x,y and z coordinates.\n",
    "        landmark3: The third landmark containing the x,y and z coordinates.\n",
    "    Returns:\n",
    "        angle: The calculated angle between the three landmarks.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Get the required landmarks coordinates.\n",
    "    x1, y1, _ = landmark1\n",
    "    x2, y2, _ = landmark2\n",
    "    x3, y3, _ = landmark3\n",
    "\n",
    "    # Calculate the angle between the three points\n",
    "    angle = math.degrees(math.atan2(y3 - y2, x3 - x2) - math.atan2(y1 - y2, x1 - x2))\n",
    "    \n",
    "    # Check if the angle is less than zero.\n",
    "    if angle < 0:\n",
    "\n",
    "        # Add 360 to the found angle.\n",
    "        angle += 360\n",
    "    \n",
    "    # Return the calculated angle.\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9215701",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Create a Function to Perform Pose Classification</font>**\n",
    "\n",
    "Now we will create a function that will be capable of classifying different yoga poses using the calculated angles of various joints. The function will be capable of identifying the following yoga poses:\n",
    "\n",
    "* **Warrior II Pose**\n",
    "* **T Pose**\n",
    "* **Tree Pose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f3a7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyPose(landmarks, output_image, display=False):\n",
    "    '''\n",
    "    This function classifies yoga poses depending upon the angles of various body joints.\n",
    "    Args:\n",
    "        landmarks: A list of detected landmarks of the person whose pose needs to be classified.\n",
    "        output_image: A image of the person with the detected pose landmarks drawn.\n",
    "        display: A boolean value that is if set to true the function displays the resultant image with the pose label \n",
    "        written on it and returns nothing.\n",
    "    Returns:\n",
    "        output_image: The image with the detected pose landmarks drawn and pose label written.\n",
    "        label: The classified pose label of the person in the output_image.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Initialize the label of the pose. It is not known at this stage.\n",
    "    label = 'Unknown Pose'\n",
    "\n",
    "    # Specify the color (Red) with which the label will be written on the image.\n",
    "    color = (0, 0, 255)\n",
    "    \n",
    "    # Calculate the required angles.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Get the angle between the left shoulder, elbow and wrist points. \n",
    "    left_elbow_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value])\n",
    "    \n",
    "    # Get the angle between the right shoulder, elbow and wrist points. \n",
    "    right_elbow_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value],\n",
    "                                       landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value],\n",
    "                                       landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value])   \n",
    "    \n",
    "    # Get the angle between the left elbow, shoulder and hip points. \n",
    "    left_shoulder_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value],\n",
    "                                         landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value],\n",
    "                                         landmarks[mp_pose.PoseLandmark.LEFT_HIP.value])\n",
    "\n",
    "    # Get the angle between the right hip, shoulder and elbow points. \n",
    "    right_shoulder_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value],\n",
    "                                          landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value],\n",
    "                                          landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value])\n",
    "\n",
    "    # Get the angle between the left hip, knee and ankle points. \n",
    "    left_knee_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.LEFT_HIP.value],\n",
    "                                     landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value],\n",
    "                                     landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value])\n",
    "\n",
    "    # Get the angle between the right hip, knee and ankle points \n",
    "    right_knee_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value])\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if it is the warrior II pose or the T pose.\n",
    "    # As for both of them, both arms should be straight and shoulders should be at the specific angle.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if the both arms are straight.\n",
    "    if left_elbow_angle > 165 and left_elbow_angle < 195 and right_elbow_angle > 165 and right_elbow_angle < 195:\n",
    "\n",
    "        # Check if shoulders are at the required angle.\n",
    "        if left_shoulder_angle > 80 and left_shoulder_angle < 110 and right_shoulder_angle > 80 and right_shoulder_angle < 110:\n",
    "\n",
    "    # Check if it is the warrior II pose.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            # Check if one leg is straight.\n",
    "            if left_knee_angle > 165 and left_knee_angle < 195 or right_knee_angle > 165 and right_knee_angle < 195:\n",
    "\n",
    "                # Check if the other leg is bended at the required angle.\n",
    "                if left_knee_angle > 90 and left_knee_angle < 120 or right_knee_angle > 90 and right_knee_angle < 120:\n",
    "\n",
    "                    # Specify the label of the pose that is Warrior II pose.\n",
    "                    label = 'Warrior II Pose' \n",
    "                        \n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if it is the T pose.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "            # Check if both legs are straight\n",
    "            if left_knee_angle > 160 and left_knee_angle < 195 and right_knee_angle > 160 and right_knee_angle < 195:\n",
    "\n",
    "                # Specify the label of the pose that is tree pose.\n",
    "                label = 'T Pose'\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if it is the tree pose.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if one leg is straight\n",
    "    if left_knee_angle > 165 and left_knee_angle < 195 or right_knee_angle > 165 and right_knee_angle < 195:\n",
    "\n",
    "        # Check if the other leg is bended at the required angle.\n",
    "        if left_knee_angle > 315 and left_knee_angle < 335 or right_knee_angle > 25 and right_knee_angle < 45:\n",
    "\n",
    "            # Specify the label of the pose that is tree pose.\n",
    "            label = 'Tree Pose'\n",
    "                \n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if the pose is classified successfully\n",
    "    if label != 'Unknown Pose':\n",
    "        \n",
    "        # Update the color (to green) with which the label will be written on the image.\n",
    "        color = (0, 255, 0)  \n",
    "    \n",
    "    # Write the label on the output image. \n",
    "    cv2.putText(output_image, label, (10, 30),cv2.FONT_HERSHEY_PLAIN, 2, color, 2)\n",
    "    \n",
    "    # Check if the resultant image is specified to be displayed.\n",
    "    if display:\n",
    "    \n",
    "        # Display the resultant image.\n",
    "        plt.figure(figsize=[10,10])\n",
    "        plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Return the output image and the classified label.\n",
    "        return output_image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16762c47",
   "metadata": {},
   "source": [
    "Now we will utilize the function created above to perform pose classification on a few images of people and display the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583aa7a3",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Warrior II Pose</font>**\n",
    "\n",
    "The Warrior II Pose (also known as Virabhadrasana II) is the same pose that the person is making in the image above. It can be classified using the following combination of body part angles:\n",
    "\n",
    "* Around 180Â° at both elbows\n",
    "* Around 90Â° angle at both shoulders\n",
    "* Around 180Â° angle at one knee\n",
    "* Around 90Â° angle at the other knee\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489eacab",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Tree Pose</font>**\n",
    "\n",
    "Tree Pose (also known as Vrikshasana) is another yoga pose for which the person has to keep one leg straight and bend the other leg at a required angle. The pose can be classified easily using the following combination of body part angles:\n",
    "\n",
    "* Around 180Â° angle at one knee\n",
    "* Around 35Â° (if right knee) or 335Â° (if left knee) angle at the other knee\n",
    "\n",
    "Now to understand it better, you should go back to the pose classification function above to overview the classification code of this yoga pose.\n",
    "\n",
    "We will perform pose classification on a few images of people in the tree yoga pose and display the results using the same function we had created above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e572da",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">T Pose</font>**\n",
    "\n",
    "T Pose (also known as a bind pose or reference pose) is the last pose we are dealing with in this lesson. To make this pose, one has to stand up like a tree with both hands wide open as branches. The following body part angles are required to make this one:\n",
    "\n",
    "* Around 180Â° at both elbows\n",
    "* Around 90Â° angle at both shoulders\n",
    "* Around 180Â° angle at both knees\n",
    "\n",
    "You can now go back to go through the classification code of this T pose in the pose classification function created above.\n",
    "\n",
    "Now, let's test the pose classification function on a few images of the T pose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc7f9a7",
   "metadata": {},
   "source": [
    "So the function is working pretty well on all the known poses on images lets try it on an unknown pose called cobra pose (also known as Bhujangasana)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226fcd0",
   "metadata": {},
   "source": [
    "Now if you want you can extend the pose classification function to make it capable of identifying more yoga poses like the one in the image above. The following combination of body part angles can help classify this one:\n",
    "\n",
    "* Around 180Â° angle at both knees\n",
    "* Around 105Â° (if the person is facing right side) or 240Â° (if the person is facing left side) angle at both hips\n",
    "\n",
    "### **<font style=\"color:rgb(134,19,348)\">Pose Classification On Real-Time Webcam Feed</font>**\n",
    "\n",
    "Now we will test the function created above to perform the pose classification on a real-time webcam feed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2238513d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d6b5cda9db74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# Perform Pose landmark detection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlandmarks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetectPose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpose_video\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Check if the landmarks are detected.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-32247d1ace86>\u001b[0m in \u001b[0;36mdetectPose\u001b[1;34m(image, pose, display)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Perform the Pose Detection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimageRGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Retrieve the height and width of the input image.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \"\"\"\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mlandmark\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mediapipe\\python\\solution_base.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    332\u001b[0m                                      data).at(self._simulated_timestamp))\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_until_idle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m     \u001b[1;31m# Create a NamedTuple object where the field names are mapping to the graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;31m# output stream names.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup Pose function for video.\n",
    "pose_video = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, model_complexity=1)\n",
    "\n",
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Initialize a resizable window.\n",
    "cv2.namedWindow('Pose Classification', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly.\n",
    "    if not ok:\n",
    "        \n",
    "        # Continue to the next iteration to read the next frame and ignore the empty camera frame.\n",
    "        continue\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get the width and height of the frame\n",
    "    frame_height, frame_width, _ =  frame.shape\n",
    "    \n",
    "    # Resize the frame while keeping the aspect ratio.\n",
    "    frame = cv2.resize(frame, (int(frame_width * (640 / frame_height)), 640))\n",
    "    \n",
    "    # Perform Pose landmark detection.\n",
    "    frame, landmarks = detectPose(frame, pose_video, display=False)\n",
    "    \n",
    "    # Check if the landmarks are detected.\n",
    "    if landmarks:\n",
    "        \n",
    "        # Perform the Pose Classification.\n",
    "        frame, _ = classifyPose(landmarks, frame, display=False)\n",
    "    \n",
    "    # Display the frame.\n",
    "    cv2.imshow('Pose Classification', frame)\n",
    "    \n",
    "    # Wait until a key is pressed.\n",
    "    # Retreive the ASCII code of the key pressed\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # Check if 'ESC' is pressed.\n",
    "    if(k == 27):\n",
    "        \n",
    "        # Break the loop.\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object and close the windows.\n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e92151",
   "metadata": {},
   "source": [
    "As expected, the results were amazing, if you were having difficulty in making the poses you can expand the range of angles used in the classification function, but that may open up the possibility of false positives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
