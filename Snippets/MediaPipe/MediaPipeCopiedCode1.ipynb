{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYVkI6qXj2q6UbFvqL12mp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobertoAlessandri/SaveTheWorld/blob/main/MediaPipeCopiedCode1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbJ5DKUVnVfu",
        "outputId": "c1972235-0ed9-4c3b-d495-07b443d0b0d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.8.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.7 MB 56.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.1.2.30)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (0.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.11.4 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (21.2.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.4->mediapipe) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.6)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.8.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe\n",
        "\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "mpPose = mp.solutions.pose\n",
        "pose = mpPose.Pose()\n",
        "mpDraw = mp.solutions.drawing_utils # For drawing keypoints\n",
        "points = mpPose.PoseLandmark # Landmarks\n",
        "\n",
        "## here is where it will be needed to change to path\n",
        "# we'll need a dir with our positions\n",
        "path = \"DATASET/TRAIN/plank\" # enter dataset path\n",
        "data = []\n",
        "for p in points:\n",
        "  x = str(p)[13:]\n",
        "  data.append(x + \"_x\")\n",
        "  data.append(x + \"_y\")\n",
        "  data.append(x + \"_z\")\n",
        "  data.append(x + \"_vis\")\n",
        "data = pd.DataFrame(columns = data) # Empty dataset\n",
        "# ^Here the columns include the thirty-three key points that will be detected by\n",
        "# the blaze pose detector (each key point contains four attributes).\n",
        "# x and y are normalized form 0 to 1\n",
        "# z represent landmark depth with hips as the origin\n",
        "# visibility = probability that the landmark is visible in the image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GxYzrpePsZIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# iterate through the pose images individually, extracting the key points using\n",
        "# the blaze pose model and sotirng them in temporary array \"temp\"\n",
        "for img in os.listdir(path):\n",
        "\n",
        "  temp = []\n",
        "\n",
        "  img = cv2.imread(path + \"/\" + img)\n",
        "\n",
        "  imageWidth, imageHeight = img.shape[:2]\n",
        "\n",
        "  imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  blackie = np.zeros(img.shape) # Blank image\n",
        "\n",
        "  results = pose.process(imgRGB)\n",
        "\n",
        "  if results.pose_landmarks:\n",
        "\n",
        "    # mpDraw.draw_landmarks(img, results.pose_landmarks, mpPose.POSE_CONNECTIONS) #draw landmarks on image\n",
        "\n",
        "    mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
        "\n",
        "    landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "    for i,j in zip(points,landmarks):\n",
        "\n",
        "      temp = temp + [j.x, j.y, j.z, j.visibility]\n",
        "\n",
        "    data.loc[count] = temp\n",
        "\n",
        "    count +=1\n",
        "\n",
        "  cv2.imshow(\"Image\", img)\n",
        "\n",
        "  cv2.imshow(\"blackie\",blackie)\n",
        "\n",
        "  cv2.waitKey(100)\n",
        "\n",
        "# We append this temporary array as a new record in our dataset\n",
        "data.to_csv(\"dataset3.csv\") # save the data as a csv file\n",
        "\n",
        "# Blaze pose model takes RGB images, OpenCV BGR\n",
        "\n",
        "# target value = label for the ML model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "KNOBOZUWoaXU",
        "outputId": "a7a1fef1-9fee-491d-a8da-5805463cb985"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6a1cf7439911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# iterate through the pose images individually, extracting the key points using\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# the blaze pose model and sotirng them in temporary array \"temp\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DATASET/TRAIN/plank'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Classification\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "data = pd.read_csv(\"dataset3.csv\")\n",
        "# we train the dataset built on SVC with the target variable as the Y label\n",
        "X,Y = data.iloc[:,:132],data['target']\n",
        "model = SVC(kernel = 'poly')\n",
        "model.fit(X,Y)\n",
        "mpPose = mp.solutions.pose\n",
        "pose = mpPose.Pose()\n",
        "mpDraw = mp.solutions.drawing_utils\n",
        "path = \"enter image path\" # !!!\n",
        "img = cv2.imread(path)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "results = pose.process(imgRGB)\n",
        "if results.pose_landmarks:\n",
        "  landmarks = results.pose_landmarks.landmark\n",
        "  for j in landmarks:\n",
        "    temp = temp + [j.x, j.y, j.z, j.visibility]\n",
        "  y = model.predict([temp])\n",
        "  if y == 0:\n",
        "    asan = \"plank\"\n",
        "  else:\n",
        "    asan = \"goddess\"\n",
        "  print(asan)\n",
        "  cv2.putText(img, asan, (50,50), cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,0),3)\n",
        "  cv2.imshow(\"image\",img)\n",
        "\n"
      ],
      "metadata": {
        "id": "VjLoyi6MqRkL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}